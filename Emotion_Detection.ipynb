{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Sequential  # type: ignore\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten # type: ignore\n","from tensorflow.keras.optimizers import Adam  # type: ignore\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["train_data_gen=ImageDataGenerator(rescale=1./255)\n","validation_data_gen=ImageDataGenerator(rescale=1./255)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 28709 images belonging to 7 classes.\n","Found 7178 images belonging to 7 classes.\n"]}],"source":["# preprocessing train images\n","train_generator=train_data_gen.flow_from_directory(\n","     'datasets/train',   # label all images inside angry as angry and with all \n","     target_size=(48,48),\n","     batch_size=64,\n","     color_mode=\"grayscale\",\n","     class_mode='categorical'\n",")\n","\n","\n","# preprocessing test images\n","validation_generator=validation_data_gen.flow_from_directory(\n","     'datasets/test',   \n","     target_size=(48,48),\n","     batch_size=64,\n","     color_mode=\"grayscale\",\n","     class_mode='categorical'\n",")\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\kushr\\OneDrive\\Desktop\\Emotion Detection Project\\emotiondetect\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["emotion_model=Sequential()\n","emotion_model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(48,48,1)))\n","emotion_model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n","emotion_model.add(Dropout(0.25))\n","\n","emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n","emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n","emotion_model.add(Dropout(0.25))\n","\n","emotion_model.add(Flatten())\n","emotion_model.add(Dense(1024,activation='relu'))\n","emotion_model.add(Dropout(0.25))\n","emotion_model.add(Dense(7,activation='softmax'))\n","\n","emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 0.3097 - loss: 1.7272"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\kushr\\OneDrive\\Desktop\\Emotion Detection Project\\emotiondetect\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 575ms/step - accuracy: 0.3097 - loss: 1.7271 - val_accuracy: 0.3864 - val_loss: 1.5848\n","Epoch 2/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.3281 - loss: 1.5684 - val_accuracy: 0.5000 - val_loss: 1.5907\n","Epoch 3/20\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\kushr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self.gen.throw(value)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 237ms/step - accuracy: 0.3974 - loss: 1.5720 - val_accuracy: 0.4339 - val_loss: 1.4818\n","Epoch 4/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104us/step - accuracy: 0.3594 - loss: 1.6302 - val_accuracy: 0.4000 - val_loss: 1.4014\n","Epoch 5/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 220ms/step - accuracy: 0.4291 - loss: 1.4856 - val_accuracy: 0.4669 - val_loss: 1.4046\n","Epoch 6/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89us/step - accuracy: 0.4062 - loss: 1.5487 - val_accuracy: 0.3000 - val_loss: 1.7514\n","Epoch 7/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 194ms/step - accuracy: 0.4670 - loss: 1.4031 - val_accuracy: 0.4831 - val_loss: 1.3466\n","Epoch 8/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64us/step - accuracy: 0.4688 - loss: 1.2319 - val_accuracy: 0.6000 - val_loss: 1.1768\n","Epoch 9/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 221ms/step - accuracy: 0.4934 - loss: 1.3413 - val_accuracy: 0.5059 - val_loss: 1.3048\n","Epoch 10/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54us/step - accuracy: 0.4219 - loss: 1.5256 - val_accuracy: 0.4000 - val_loss: 1.4082\n","Epoch 11/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 223ms/step - accuracy: 0.5137 - loss: 1.2894 - val_accuracy: 0.5202 - val_loss: 1.2683\n","Epoch 12/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103us/step - accuracy: 0.5625 - loss: 1.1971 - val_accuracy: 0.7000 - val_loss: 1.0379\n","Epoch 13/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 205ms/step - accuracy: 0.5289 - loss: 1.2492 - val_accuracy: 0.5257 - val_loss: 1.2418\n","Epoch 14/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84us/step - accuracy: 0.5312 - loss: 1.3217 - val_accuracy: 0.7000 - val_loss: 1.0192\n","Epoch 15/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 217ms/step - accuracy: 0.5444 - loss: 1.2136 - val_accuracy: 0.5359 - val_loss: 1.2128\n","Epoch 16/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80us/step - accuracy: 0.4688 - loss: 1.3168 - val_accuracy: 0.6000 - val_loss: 1.4075\n","Epoch 17/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 223ms/step - accuracy: 0.5628 - loss: 1.1783 - val_accuracy: 0.5434 - val_loss: 1.1990\n","Epoch 18/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111us/step - accuracy: 0.6406 - loss: 1.0825 - val_accuracy: 0.9000 - val_loss: 0.6992\n","Epoch 19/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 212ms/step - accuracy: 0.5764 - loss: 1.1299 - val_accuracy: 0.5431 - val_loss: 1.1915\n","Epoch 20/20\n","\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65us/step - accuracy: 0.5469 - loss: 1.1862 - val_accuracy: 0.5000 - val_loss: 1.7602\n"]}],"source":["from tensorflow.keras.callbacks import EarlyStopping  # type: ignore\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n","\n","emotion_model_info = emotion_model.fit(\n","    train_generator,\n","    steps_per_epoch=28709 // 64,\n","    epochs=70,\n","    validation_data=validation_generator,\n","    validation_steps=7178 // 64,\n","    callbacks=[early_stopping]\n",")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" This above code is run on another device because of high processing power hence the .json and weights file is different from the mentioned below and its accuracy is higher."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["model_json=emotion_model.to_json()\n","with open(\"emotion_model.json\",'w') as json_file:\n","     json_file.write(model_json)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["emotion_model.save_weights('emotion_model.weights.h5')"]}],"metadata":{"kernelspec":{"display_name":"emotiondetect","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":2}
